{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Session 3: Alignment and Querying Systems\n",
    "\n",
    "## Alignment\n",
    "\n",
    "In this part of the session, we are trying to get the horizons corpus aligned, and have it in a similar format in which [OPUS](http://opus.nlpl.eu/) makes their parallel corpora available.\n",
    "\n",
    "```xml\n",
    "<document src=\"de\" trg=\"en\">\n",
    "    <sent_pair id=\"0\">\n",
    "        <src lang=\"de\">Der Fuchs spricht in fremden Zungen.</src>\n",
    "        <trg lang=\"en\">The fox speaks with strange tongues.</trg>\n",
    "    </sent_pair>\n",
    "</document>\n",
    "```\n",
    "\n",
    "If we use this format for the english and the german part of the horizons corpus, we gain some information, but we also lose some. Textual coherence that spans multiple sentences doesn't necessarily get preserved. We also have to assume that in the articles, there are not only 1:1-relationships between the german and the english sentences. Maybe a german sentence gets paraphrased by two or more english sentences. This format doesn't facilitate n:n-relationships. Nonetheless it is useful, maybe as a test set for a MT-application, or as the grounds of a statistically computed dictionary.\n",
    "\n",
    "### Aligning\n",
    "\n",
    "There are different alignment-tasks, namely document, sentence and word alignment. They all require to some extent different algorithms and techniques. Today we are only focusing on sentence alignment. The document alignment has already been done when we crawled the web for the corpus: We were able to use the semantic links between the articles to identify corresponding documents.\n",
    "\n",
    "To align sentences, there can be different approaches:\n",
    "\n",
    "- Length-based: [Gale-and-Church-Algorithm](https://www.nltk.org/_modules/nltk/translate/gale_church.html)\n",
    "- Length-and-Dictionary-based: [hunalign](https://github.com/danielvarga/hunalign)\n",
    "- MT-based: [Bleualign](https://github.com/rsennrich/Bleualign)\n",
    "\n",
    "#### Bleualign\n",
    "\n",
    "We are going to use Bleualign in this session. Bleualign takes as an input the two texts we want to align, as well as a self-made translation of one of the texts. It then uses the similarity between this secondary text and the text in the same language to align the two primary texts. The great thing is, this secondary translation doesn't have to be that good, it just hast to be alright. Therefore, we can easily have a MT-tool do this task.\n",
    "\n",
    "<img src=\"img/bleualign.png\" alt=\"bleualign\" style=\"width: 500px;\"/>\n",
    "\n",
    "The metric it uses to determine the similarity between the translation is BLEU, a score widely used in MT to evaluate the quality of an automatic translation.\n",
    "\n",
    "The core idea behind it is to compare a candidate, an automatic translation, to one or more reference translations. This formula here is a simplified BLEU-score, only taking Unigrams and one Reference Sentence into account:\n",
    "\n",
    "$$\n",
    "BLEU_{simple} = \\frac{\\sum_{\\text{Unigram} \\in \\text{Candidate}}^{}\\text{matched(Unigram)}}{\\text{Length(Candidate)}}\n",
    "$$\n",
    "\n",
    "Let's use an example:\n",
    "\n",
    "|x|t1|t2|t3|t4|t5|t6|\n",
    "|-|--|--|--|--|--|--|\n",
    "|Candidate|the|cat|runs|from|the|dog|\n",
    "|Reference|the|cat|flees|from|a|dog|\n",
    "|Reference 2|the|cat|-|-|-|-|-|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{matched(the)}=1\\\\\n",
    "\\text{matched(cat)}=1\\\\\n",
    "\\text{matched(runs)}=0\\\\\n",
    "\\text{matched(from)}=1\\\\\n",
    "\\text{matched(dog)}=1\\\\\\\\\n",
    "BLEU_{simple} = \\frac{4}{6} =0.66\n",
    "$$\n",
    "\n",
    "Now, you might see a shortcoming of this metric. If the Candidate is really short, maybe has only one token in it, it gets a really good score, because the metric is normalized over the Candidates length. For example, if our Candidate only consisted of \"cat\", it would get a perfect score, even though it is far from the Reference.\n",
    "\n",
    "That is why the BLEU score additionally introduces a so called *Brevity Penalty*. If a Candidate is shorter than the Reference, it automatically gets a lower score. This might be a rather brute approach, but it has proven to match rather well with our expectations of good translations.\n",
    "\n",
    "### Doing the alignment\n",
    "\n",
    "For now, I only want to align five articles as a proof-of-concept.\n",
    "\n",
    "First, we need to get the articles into the format we need. Beforehand, I cut out five articles from the corpus, which looks like this:\n",
    "\n",
    "```xml\n",
    "<corpus>\n",
    "    <article id=\"a1\" issue=\"120\" lang=\"de\">\n",
    "        <div class=\"title\">Titel</div>\n",
    "        <div class=\"abstract\">Einführung</div>\n",
    "        <div>Satz 1. Satz 2. Satz 3.</div>\n",
    "        <div>Satz 4. Satz 5.</div>\n",
    "    </article>\n",
    "</corpus>\n",
    "```\n",
    "\n",
    "Bleualign expects the documents to have to have an id as a filename, and a suffix corresponding to their status. For example, the three files needed to create an alignment could be called `0.en`, `0.de` and `0.trans`. Additionally Bleualign expects a sentence per line.\n",
    "\n",
    "First I define a function that lets me print a directory tree, so I don't need to switch editors to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the real deal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from lxml import etree\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Path(\"/my/directory\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def my_parse(lang):\n",
    "    filename = Path(f\"corpus_cutouts/{lang}_corpus_cutout.xml\")\n",
    "    \n",
    "    Path(\"./alignment_dir\").mkdir(parents=True, exist_ok=True)\n",
    "    # Iterative Parsing, great for xml-files that don't fit into RAM \n",
    "    for _, article in etree.iterparse(str(filename), tag=\"article\"):\n",
    "        art_id = article.get(\"id\")\n",
    "        \n",
    "        with open(f\"alignment_dir/{art_id[1:]}.{lang}\", \"w\", encoding=\"utf8\") as outfile:\n",
    "            for elem in article.xpath(\"./div[@class='title']|./div[@class='abstract']|./div[not(@class)]\"):\n",
    "                text = elem.text\n",
    "                if lang == \"de\":\n",
    "                    \n",
    "                    # NLTKs sentence segmentizer works better than spacys if there are quotation marks in the text\n",
    "                    sent_text = nltk.sent_tokenize(text, language='german')\n",
    "                elif lang == \"en\":\n",
    "                    sent_text = nltk.sent_tokenize(text, language='english')\n",
    "                    \n",
    "                for sent in sent_text:\n",
    "                \n",
    "                    text = re.split(r'\\.»', sent)\n",
    "                    \n",
    "                    if len(text)>1:\n",
    "                        text[0] = text[0] + \".»\"\n",
    "\n",
    "                    for sent in text:\n",
    "                        if sent == \"\":\n",
    "                            continue\n",
    "    \n",
    "                        tokenized_sent = nltk.word_tokenize(sent)\n",
    "                        outfile.write(f\"{' '.join(tokenized_sent)}\\n\")\n",
    "            # return here to only parse one file\n",
    "            return\n",
    "    print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parse(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parse(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "    Exercise Session 3.ipynb\n",
      "    .ipynb_checkpoints/\n",
      "        Exercise Session 3-checkpoint.ipynb\n",
      "    alignment_dir/\n",
      "        160.de\n",
      "        160.en\n",
      "    corpus_cutouts/\n",
      "        de_corpus_cutout.xml\n",
      "        en_corpus_cutout.xml\n",
      "    img/\n",
      "        bleualign.png\n"
     ]
    }
   ],
   "source": [
    "list_files(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can start to translate the texts of one of the languages. For this, I am using the [googletrans](https://pypi.org/project/googletrans/)-module, which serves as a Python-interface for Google Translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "def obtain_translation(artid):\n",
    "    inpath = Path(f\"alignment_dir/{artid}.de\")\n",
    "    outpath = Path(f\"alignment_dir/{artid}.trans\")\n",
    "\n",
    "    translator = Translator()\n",
    "\n",
    "    with open(inpath, \"r\", encoding=\"utf8\") as infile:\n",
    "        with open(outpath, \"w\", encoding=\"utf8\") as outfile:\n",
    "            translations = []\n",
    "            for line in infile:\n",
    "                translation = translator.translate(line, src=\"de\", dest=\"en\").text\n",
    "                outfile.write(f\"{translation}\\n\")\n",
    "                \n",
    "    print(f\"Finished article {artid}\")\n",
    "\n",
    "import translate\n",
    "    \n",
    "def no_google_obtain_translation(artid):\n",
    "    inpath = Path(f\"alignment_dir/{artid}.de\")\n",
    "    outpath = Path(f\"alignment_dir/{artid}.trans\")\n",
    "\n",
    "    with open(inpath, \"r\", encoding=\"utf8\") as infile:\n",
    "        with open(outpath, \"w\", encoding=\"utf8\") as outfile:\n",
    "            translator = translate.Translator(to_lang=\"en\", from_lang=\"de\")\n",
    "            for line in infile:\n",
    "                trans_line = translator.translate(line)\n",
    "                outfile.write(trans_line + \"\\n\")\n",
    "                \n",
    "    print(f\"Finished article {artid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be aware that the Google Translate API has restrictions on how many lines you are allowed translate. I found, that I can only run this script around once per day. Otherwise, it returns a `JSONDecodeError`. For larger corpora, you either need to do some VPN-magic, pay for a translation service or use something local (or on the CL-server). If this isn't in your power,  a length-based algorithm like hunalign is probably the better choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished article 160\n"
     ]
    }
   ],
   "source": [
    "#for i in range(160,165):\n",
    "#    obtain_translation(str(i))\n",
    "\n",
    "obtain_translation(160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is using Bleualign. It's functionality could be imported to your own script, but it's easier to just use it on the command line.\n",
    "\n",
    "I am going to use the following call, which enables batch-processing:\n",
    "\n",
    "`python path/to/batch_align.py aligned_dir de en trans`\n",
    "\n",
    "This creates two new files per document pair, where there are only matching sentence pairs. Sentences that didn't match up, are discarded. For some use-cases this might not be desired, but for our goals it's just what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files(\"alignment_dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these `.aligned`-files, the lines correspond to each other. The sentence on line number 36 in file `163.de.aligned` should be the translation of the sentence on the same line number in `163.en.aligned`. \n",
    "\n",
    "### Creating the final format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I create a generator over the sentence pairs. This makes it alot easier to handle later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_iterator(directory, artid):\n",
    "    with open(Path(f\"{directory}/{artid}.de.aligned\"), \"r\", encoding=\"utf8\") as defile:\n",
    "        with open(Path(f\"{directory}/{artid}.en.aligned\"), \"r\", encoding=\"utf8\") as enfile:\n",
    "            for de, en in zip(defile, enfile):\n",
    "                yield (de.strip(), en.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this XML-file, I am going to use the `xmlfile`-Class of lxml. Before, we always built the XML-file we wanted completely in memory, before printing it to a file. This works well for smaller amounts of data, but can get us in trouble if our RAM is not big enough. `xmlfile` allows us to build *and write* our tree continously. This means, that after we've processed one sentence pair, we can forget about it, because it's already written to the file.\n",
    "\n",
    "Of course, this is once again not necessary for our example, but if we had thousands of articles, this technique can prove useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opus_xml(source_dir, target, start, end):\n",
    "    with etree.xmlfile(target, encoding=\"utf8\") as xf:\n",
    "        with xf.element(\"document\", {\"src\":\"de\",\"trg\":\"en\"}):\n",
    "            \n",
    "            id_counter = 0\n",
    "            \n",
    "            for i in range(start, end+1):\n",
    "                \n",
    "                for de, en in pair_iterator(source_dir, str(i)):\n",
    "                    sent_pair = etree.Element(\"sent_pair\")\n",
    "                    sent_pair.attrib[\"id\"] = str(id_counter)\n",
    "                    \n",
    "                    source = etree.SubElement(sent_pair, \"source\")\n",
    "                    source.attrib[\"lang\"] = \"de\"\n",
    "                    source.text = de\n",
    "                    \n",
    "                    target = etree.SubElement(sent_pair, \"target\")\n",
    "                    target.attrib[\"lang\"] = \"en\"\n",
    "                    target.text = en\n",
    "                    \n",
    "                    xf.write(sent_pair)\n",
    "                    \n",
    "                    id_counter += 1\n",
    "                    \n",
    "create_opus_xml(\"alignment_dir\", \"opus.xml\", 160, 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Corpora\n",
    "\n",
    "A corpus is not defined by it's format. Therefore, a multitude of formats and possibilities to query these formats exists.\n",
    "\n",
    "We have already gotten to know the three-rowed verticalized format in exercise session 1, which we could query by using UNIX-tools.\n",
    "\n",
    "I want to present two additional ways to query corpora:\n",
    "\n",
    "- [Corpus Workbench](https://cqpweb.lancs.ac.uk/): The Corpus Workbench is a tool for corpus linguists. It allows processing large corpora, is rather fast and facilitates the most common techniques of corpus linguistics, such as the Analysis of Collocations, Keywords and Distributions.\n",
    "    - [Language and Encoding Documentation](http://cwb.sourceforge.net/documentation.php)\n",
    "    \n",
    "    \n",
    "- [multilingwis2](https://pub.cl.uzh.ch/projects/sparcling/multilingwis2.demo/): Multlingwis2 is a tool developped here in Zurich. It is an explorative querying tool for parallel (and *multi*parallel) Corpora. \n",
    "    - [The corpora behind](https://pub.cl.uzh.ch/wiki/public/pacoco/start)\n",
    "\n",
    "They both offer a rather high-level Interface to interact with corpora. The complexity of the data beneath is abstracted and can be navigated more easily, even by people without a computational background. Of course, this disallows some forms of querying, but this is a tradeoff that is always prevalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 64-bit",
   "language": "python",
   "name": "python37264bit3fd49fa2c9514b069bc4c31befd88188"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
